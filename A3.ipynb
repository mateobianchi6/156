{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Mini-batch Stochastic Gradient Descent (SGD)\n",
    "def mini_batch_SGD(X, t, w, b, learning_rate, num_epochs, batch_size):\n",
    "    m = X.shape[0]  # Number of training examples\n",
    "    cost_history = []  # To store the cost at each iteration\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at the beginning of each epoch\n",
    "        indices = np.arange(m)\n",
    "        np.random.shuffle(indices)\n",
    "        X_shuffled = X[indices]\n",
    "        t_shuffled = t[indices]\n",
    "\n",
    "        # Process each mini-batch\n",
    "        for i in range(0, m, batch_size):\n",
    "            # Create mini-batch of size batch_size\n",
    "            X_batch = X_shuffled[i:i+batch_size]\n",
    "            t_batch = t_shuffled[i:i+batch_size]\n",
    "\n",
    "            # Compute gradients on the mini-batch\n",
    "            dw, db = compute_gradients(X_batch, t_batch, w, b)\n",
    "            #print(dw.shape)\n",
    "            # Update parameters using the gradients from the mini-batch\n",
    "            w -= learning_rate * dw\n",
    "            b -= learning_rate * db\n",
    "\n",
    "        # After processing all mini-batches, compute the cost over the entire dataset\n",
    "        cost = compute_cost(X, t, w, b)\n",
    "        cost_history.append(cost)\n",
    "\n",
    "        # Print cost every 100 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}/{num_epochs} - Cost: {cost}\")\n",
    "\n",
    "    return w, b, cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1000 - Cost: 8.440561161060444\n",
      "Epoch 100/1000 - Cost: 2.197929497555975\n",
      "Epoch 200/1000 - Cost: 7.160448505501645\n",
      "Epoch 300/1000 - Cost: 1.8039815717698193\n",
      "Epoch 400/1000 - Cost: 2.8626697380985724\n",
      "Epoch 500/1000 - Cost: 2.04175106725459\n",
      "Epoch 600/1000 - Cost: 2.1355859325194526\n",
      "Epoch 700/1000 - Cost: 1.8273789314984392\n",
      "Epoch 800/1000 - Cost: 2.2982238971856432\n",
      "Epoch 900/1000 - Cost: 4.71077846282286\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sigmoid function to compute hypothesis\n",
    "def sigmoid(z):\n",
    "    z = np.clip(z, -500, 500) # to avoid numerical instability\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Cost function (cross-entropy loss)\n",
    "def compute_cost(X, t, w, b):\n",
    "    m = len(X)\n",
    "    z = np.dot(X, w) + b  # Compute z = Xw + b\n",
    "    y = sigmoid(z)\n",
    "    \n",
    "    epsilon = 1e-10  # Small constant to avoid log(0)\n",
    "    y = np.clip(y, epsilon, 1 - epsilon) # to avoid numerical instability\n",
    "    \n",
    "    cost = -(1/m) * np.sum(t * np.log(y) + (1 - t) * np.log(1 - y))\n",
    "    return cost\n",
    "\n",
    "# Compute gradients for weights and bias\n",
    "def compute_gradients(X, t, w, b):\n",
    "    m = len(X)\n",
    "    z = np.dot(X, w) + b  # Compute z = Xw + b\n",
    "    y = sigmoid(z).reshape(-1,1) # should fix issue with dw dimensions\n",
    "    \n",
    "    # Gradients for weights and bias\n",
    "    dw = (1/m) * np.dot(X.T, (y - t))  # Gradient w.r.t. weights\n",
    "    db = (1/m) * np.sum(y - t)          # Gradient w.r.t. bias\n",
    "    return dw, db ##for some reason here dw's dimensions get messed up\n",
    "\n",
    "# Mini-batch Stochastic Gradient Descent (SGD)\n",
    "def mini_batch_SGD(X, t, w, b, learning_rate, num_epochs, batch_size):\n",
    "    m = X.shape[0]  # Number of training examples\n",
    "    cost_history = []  # To store the cost at each iteration\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at the beginning of each epoch\n",
    "        indices = np.arange(m)\n",
    "        np.random.shuffle(indices)\n",
    "        X_shuffled = X[indices]\n",
    "        t_shuffled = t[indices]\n",
    "\n",
    "        # Process each mini-batch\n",
    "        for i in range(0, m, batch_size):\n",
    "            # Create mini-batch of size batch_size\n",
    "            X_batch = X_shuffled[i:i+batch_size]\n",
    "            t_batch = t_shuffled[i:i+batch_size]\n",
    "\n",
    "            # Compute gradients on the mini-batch\n",
    "            dw, db = compute_gradients(X_batch, t_batch, w, b)\n",
    "            #print(dw.shape)\n",
    "            # Update parameters using the gradients from the mini-batch\n",
    "            w -= learning_rate * dw\n",
    "            b -= learning_rate * db\n",
    "\n",
    "        # After processing all mini-batches, compute the cost over the entire dataset\n",
    "        cost = compute_cost(X, t, w, b)\n",
    "        cost_history.append(cost)\n",
    "\n",
    "        # Print cost every 100 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}/{num_epochs} - Cost: {cost}\")\n",
    "\n",
    "    return w, b, cost_history\n",
    "\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "\n",
    "# Add the target column\n",
    "df['target'] = data.target\n",
    "\n",
    "# Suppose X is your feature set and y is your target\n",
    "X = df.drop(\"target\", axis=1) \n",
    "y = df[\"target\"]\n",
    "# First, split the data into training + validation and test sets (80% train, 20% test)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Next, split the training + validation set into training and validation sets (e.g., 80% train, 20% val)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2 of the original data\n",
    "\n",
    "# Now you have X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\"\"\"\n",
    "print (len(X_train))\n",
    "print (len(X_val))\n",
    "print (len(X_test))\n",
    "\"\"\"\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# Initialize weights randomly using a Gaussian distribution\n",
    "# Mean = 0, Standard deviation = 0.01, shape = (n_features,)\n",
    "w = np.random.normal(loc=0.0, scale=0.01, size=(n_features, 1))\n",
    "\n",
    "# Initialize bias as zero or you could also initialize it randomly\n",
    "b = 0\n",
    "\n",
    "X_train_np = X_train.to_numpy()\n",
    "y_train_np = y_train.to_numpy()\n",
    "y_train_np = y_train_np.reshape(-1,1)\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "batch_size = 32\n",
    "\n",
    "w_final, b_final, cost_history = mini_batch_SGD(X_train_np, y_train_np, w, b, learning_rate, num_epochs, batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
